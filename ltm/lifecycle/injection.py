# MIT License
# Copyright (c) 2025 Matt / Grain Ecosystem

"""
Memory injection for session start.

Retrieves relevant memories and formats them for context injection.
Respects the 10% context budget.
"""

from functools import lru_cache
from typing import Optional, TypedDict

import tiktoken

from ltm.core import (
    Memory, MemoryBlock, RegionType,
    Agent, Project,
    verify_signature, should_verify
)
from ltm.storage import MemoryStore


class InjectionStats(TypedDict):
    """Statistics about memory injection."""
    agent_memories: int
    project_memories: int
    total: int
    budget_tokens: int
    priority_counts: dict[str, int]  # CRITICAL, HIGH, MEDIUM, LOW


# Default values (can be overridden via ~/.ltm/config.json)
DEFAULT_CONTEXT_SIZE = 200_000  # tokens (Claude's standard context window)
MEMORY_BUDGET_PERCENT = 0.10   # 10% of context


def _get_budget_config() -> tuple[int, float]:
    """Get budget settings from config."""
    from ltm.core.config import get_config
    config = get_config()
    return config.budget.context_size, config.budget.context_percent


@lru_cache(maxsize=4)
def _get_encoder(model: str):
    """Cache tiktoken encoders for reuse."""
    return tiktoken.get_encoding(model)


def count_tokens(text: str, model: str = "cl100k_base") -> int:
    """Count tokens in text using tiktoken."""
    try:
        enc = _get_encoder(model)
        return len(enc.encode(text))
    except Exception:
        # Fallback: rough estimate of 4 chars per token
        return len(text) // 4


def estimate_tokens(text: str) -> int:
    """Fast approximate token count (~4 chars per token)."""
    return len(text) // 4


def get_memory_tokens(memory: Memory) -> int:
    """
    Get token count for a memory's DSL representation.

    Uses cached token_count if available, otherwise falls back to
    fast approximation. The accurate count is calculated on save.
    """
    if memory.token_count is not None:
        return memory.token_count
    # Fast fallback for memories without cached count
    return estimate_tokens(memory.to_dsl() + "\n")


def calculate_token_count(memory: Memory) -> int:
    """
    Calculate accurate token count for a memory using tiktoken.

    This should be called when saving a memory to cache the count.
    Returns the token count for the memory's DSL representation.
    """
    memory_dsl = memory.to_dsl() + "\n"
    return count_tokens(memory_dsl)


def ensure_token_count(memory: Memory) -> None:
    """
    Ensure a memory has its token_count cached.

    Calculates and sets token_count if not already set.
    Call this before saving a memory.
    """
    if memory.token_count is None:
        memory.token_count = calculate_token_count(memory)


def get_memory_budget(context_size: Optional[int] = None) -> int:
    """
    Calculate token budget for memories.

    Uses config values if context_size not specified.
    """
    if context_size is None:
        context_size, percent = _get_budget_config()
        return int(context_size * percent)
    # Fallback to default percent if only size specified
    return int(context_size * MEMORY_BUDGET_PERCENT)


class MemoryInjector:
    """
    Handles memory retrieval and injection for session start.

    Retrieves memories for the current agent and project, formats them
    in the compact DSL, and respects token budget constraints.
    """

    def __init__(
        self,
        store: Optional[MemoryStore] = None,
        context_size: int = DEFAULT_CONTEXT_SIZE
    ):
        self.store = store or MemoryStore()
        self.budget = get_memory_budget(context_size)

    def inject(
        self,
        agent: Agent,
        project: Optional[Project] = None
    ) -> str:
        """
        Get formatted memories for injection into context.

        Retrieves both AGENT region memories and PROJECT region memories
        (if a project is specified), formats them in DSL, and ensures
        we stay within token budget.

        Args:
            agent: The current agent
            project: The current project (optional)

        Returns:
            Formatted memory block as a string, or empty string if no memories
        """
        memories: list[Memory] = []

        # Get AGENT region memories (cross-project)
        agent_memories = self.store.get_memories_for_agent(
            agent_id=agent.id,
            region=RegionType.AGENT,
            include_superseded=False
        )
        memories.extend(agent_memories)

        # Get PROJECT region memories (project-specific)
        if project:
            project_memories = self.store.get_memories_for_agent(
                agent_id=agent.id,
                region=RegionType.PROJECT,
                project_id=project.id,
                include_superseded=False
            )
            memories.extend(project_memories)

        if not memories:
            return ""

        # Sort by importance: CRITICAL first, then by recency
        memories = self._prioritize_memories(memories)

        # Build memory block within budget
        block = MemoryBlock(
            agent_name=agent.name,
            project_name=project.name if project else None,
            memories=[]
        )

        # Header/footer overhead (use estimate - it's small and constant)
        current_tokens = estimate_tokens(f"[LTM:{agent.name}]\n[/LTM]")

        for memory in memories:
            # Verify signature if agent has signing key and memory is signed
            if should_verify(memory, agent):
                if not verify_signature(memory, agent.signing_key):  # type: ignore
                    # Mark as untrusted - will show âš  in DSL
                    memory.signature_valid = False
                else:
                    memory.signature_valid = True

            # Use cached token count (fast) or estimate (also fast)
            memory_tokens = get_memory_tokens(memory)

            if current_tokens + memory_tokens <= self.budget:
                block.memories.append(memory)
                current_tokens += memory_tokens
                # Update last_accessed
                memory.touch()
                self.store.save_memory(memory)
            else:
                # Budget exceeded, stop adding memories
                break

        if not block.memories:
            return ""

        return block.to_dsl()

    def _prioritize_memories(self, memories: list[Memory]) -> list[Memory]:
        """
        Prioritize memories for injection.

        Priority order:
        1. Impact level (CRITICAL > HIGH > MEDIUM > LOW)
        2. Recency (newer first within same impact)
        3. Kind (EMOTIONAL first, as it shapes interaction style)
        """
        impact_order = {
            "CRITICAL": 0,
            "HIGH": 1,
            "MEDIUM": 2,
            "LOW": 3
        }
        kind_order = {
            "EMOTIONAL": 0,  # Most important for interaction style
            "ARCHITECTURAL": 1,
            "LEARNINGS": 2,
            "ACHIEVEMENTS": 3
        }

        def sort_key(m: Memory) -> tuple:
            return (
                impact_order.get(m.impact.value, 99),
                kind_order.get(m.kind.value, 99),
                -m.created_at.timestamp()  # Negative for descending (newer first)
            )

        return sorted(memories, key=sort_key)

    def get_stats(self, agent: Agent, project: Optional[Project] = None) -> InjectionStats:
        """Get statistics about memories for this agent/project."""
        agent_memories = self.store.get_memories_for_agent(
            agent_id=agent.id,
            region=RegionType.AGENT,
            include_superseded=False
        )

        project_memories: list[Memory] = []
        if project:
            project_memories = self.store.get_memories_for_agent(
                agent_id=agent.id,
                region=RegionType.PROJECT,
                project_id=project.id,
                include_superseded=False
            )

        # Count by priority
        priority_counts = {"CRITICAL": 0, "HIGH": 0, "MEDIUM": 0, "LOW": 0}
        for memory in agent_memories + project_memories:
            priority_counts[memory.impact.value] += 1

        return {
            "agent_memories": len(agent_memories),
            "project_memories": len(project_memories),
            "total": len(agent_memories) + len(project_memories),
            "budget_tokens": self.budget,
            "priority_counts": priority_counts
        }
